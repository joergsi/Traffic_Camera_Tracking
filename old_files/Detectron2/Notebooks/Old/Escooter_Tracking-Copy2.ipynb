{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4PF3JBkGlNY"
   },
   "source": [
    "## Traffic Camera Tracking\n",
    "\n",
    "Main goals of this project is to track escooters, pedestrians and cyclists through a traffic intersection and record informations regarding their time of passing and the directions. This project is a part of **SaveNoW**, where all these details are used to create a digital twin of Ingolstadt for simulation purposes.\n",
    "\n",
    "So far only escooters have been taken into account of this project. When all the code works, it won't be difficult to retrain the model with enough annotations of cyclists and pedestrians and modify a few lines of code to accomodate them.\n",
    "\n",
    "_For further updates on the project, follow this notion page: https://www.notion.so/Project-Traffic-Camera-tracking-15a9bb984c1341369ad40562610dc83f_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "\n",
    "This project depends heavily on **Detectron2** and **PyTorch** for running a MASK R-CNN Model for instance segmentation. Installing them is really easy for Linux Machines but for Windows 10, some workaround is required (_I have mentioned about this in my notion page_). PyTorch can be installed from their main website. The installations are not part of this notebook and must be done separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmP1ykzXGZCU",
    "outputId": "df559e97-4114-40d6-a250-5cfbeac3cfa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0 True\n"
     ]
    }
   ],
   "source": [
    "# install dependencies: \n",
    "!pip install pyyaml==5.1\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USnAkQi2GyPU"
   },
   "source": [
    "## Importing the required packages and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pW5CUJtOGug1"
   },
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from os import path\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7TI8Lj96sgf"
   },
   "source": [
    "## Input Data\n",
    "\n",
    "The folder structure, in which all the clips and the associated annotations must be placed\n",
    "\n",
    "**!! Images directory is not required. It is created in the next code cell**\n",
    "\n",
    "```\n",
    "input/\n",
    "│\n",
    "└─── 1/\n",
    "│       1.mp4\n",
    "│       1_Annotations.json\n",
    "|       images/\n",
    "|            frame_000001.jpg\n",
    "|            frame_000002.jpg and so on\n",
    "|\n",
    "└─── 2/\n",
    "│       2.mp4\n",
    "│       2_Annotations.json\n",
    "|       images/\n",
    "|            frame_000001.jpg\n",
    "|            frame_000002.jpg and so on\n",
    ".\n",
    ".\n",
    ".\n",
    "│   \n",
    "└─── {Clip_Number}/\n",
    "        {Clip_Number}.mp4\n",
    "        {Clip_Number}_Annotations.json\n",
    "        images/\n",
    "             frame_000001.jpg\n",
    "             frame_000002.jpg and so on\n",
    "```\n",
    "\n",
    "## Splitting the clips into frames\n",
    "In the below code cell, we are using FFMPEG Commands to split the clip into images for training the MASK R-CNN Model.\n",
    "\n",
    "**!! Note: Install FFMPEG first. If not, atleast have a FFMPEG Executable file in the same directory as running this notebook, otherwise it won't work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJztU9NdLZOh",
    "outputId": "9206108b-eebe-434b-ca4a-228ef2908536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balaji\\Desktop\\Traffic_Camera_Tracking\\Main_Code\n",
      "D:\\Project_Escooter_Tracking\\input\n",
      "D:\\Project_Escooter_Tracking\\input\\16\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\16\\16.mp4 D:\\Project_Escooter_Tracking\\input\\16\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\18\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\18\\18.mp4 D:\\Project_Escooter_Tracking\\input\\18\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\2\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\2\\2.mp4 D:\\Project_Escooter_Tracking\\input\\2\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\20\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\20\\20.mp4 D:\\Project_Escooter_Tracking\\input\\20\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\21\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\21\\21.mp4 D:\\Project_Escooter_Tracking\\input\\21\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\22\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\22\\22.mp4 D:\\Project_Escooter_Tracking\\input\\22\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\23\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\23\\23.mp4 D:\\Project_Escooter_Tracking\\input\\23\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\24\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\24\\24.mp4 D:\\Project_Escooter_Tracking\\input\\24\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\25\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\25\\25.mp4 D:\\Project_Escooter_Tracking\\input\\25\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\26\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\26\\26.mp4 D:\\Project_Escooter_Tracking\\input\\26\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\27\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\27\\27.mp4 D:\\Project_Escooter_Tracking\\input\\27\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\28\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\28\\28.mp4 D:\\Project_Escooter_Tracking\\input\\28\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\29\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\29\\29.mp4 D:\\Project_Escooter_Tracking\\input\\29\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\3\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\3\\3.mp4 D:\\Project_Escooter_Tracking\\input\\3\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\30\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\30\\30.mp4 D:\\Project_Escooter_Tracking\\input\\30\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\31\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\31\\31.mp4 D:\\Project_Escooter_Tracking\\input\\31\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\32\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\32\\32.mp4 D:\\Project_Escooter_Tracking\\input\\32\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\33\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\33\\33.mp4 D:\\Project_Escooter_Tracking\\input\\33\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\34\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\34\\34.mp4 D:\\Project_Escooter_Tracking\\input\\34\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\35\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\35\\35.mp4 D:\\Project_Escooter_Tracking\\input\\35\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\36\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\36\\36.mp4 D:\\Project_Escooter_Tracking\\input\\36\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\37\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\37\\37.mp4 D:\\Project_Escooter_Tracking\\input\\37\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\38\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\38\\38.mp4 D:\\Project_Escooter_Tracking\\input\\38\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\39\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\39\\39.mp4 D:\\Project_Escooter_Tracking\\input\\39\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\4\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\4\\4.mp4 D:\\Project_Escooter_Tracking\\input\\4\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\40\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\40\\40.mp4 D:\\Project_Escooter_Tracking\\input\\40\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\41\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\41\\41.mp4 D:\\Project_Escooter_Tracking\\input\\41\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\42\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\42\\42.mp4 D:\\Project_Escooter_Tracking\\input\\42\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\43\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\43\\43.mp4 D:\\Project_Escooter_Tracking\\input\\43\\images\\frame_%06d.png\n",
      "D:\\Project_Escooter_Tracking\\input\\44\n",
      "ffmpeg -i D:\\Project_Escooter_Tracking\\input\\44\\44.mp4 D:\\Project_Escooter_Tracking\\input\\44\\images\\frame_%06d.png\n"
     ]
    }
   ],
   "source": [
    "# main_path refers to current working directory\n",
    "main_path = os.getcwd()\n",
    "print(main_path)\n",
    "# input_path refers to the folder containing the clips folders (as mentioned above in the diagram)\n",
    "#input_path = main_path + '/input'\n",
    "input_path = os.path.abspath(r'D:\\Project_Escooter_Tracking\\input')\n",
    "print(input_path)\n",
    "\n",
    "for dir in os.listdir(input_path):\n",
    "  clip_path = input_path + f'\\\\{dir}'\n",
    "  print(clip_path)\n",
    "  \n",
    "  image_path = clip_path + '\\\\images'\n",
    "  os.system(f'mkdir {image_path}')\n",
    "\n",
    "  labels_path = image_path + '\\\\labels'\n",
    "  os.system(f'mkdir {labels_path}')\n",
    "\n",
    "  clip_path += f'\\\\{dir}.mp4'\n",
    "\n",
    "  ffmpeg_command = 'ffmpeg -i ' + clip_path + \" \" + image_path + '\\\\frame_%06d.png'\n",
    "  print(ffmpeg_command)\n",
    "  os.system(ffmpeg_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGUbSvjLHOan"
   },
   "source": [
    "## Reading, manipulating and checking the annotations on the images\n",
    "\n",
    "### This code section is partly derived from Check_Dataset.ipynb and ReadJSON.py. \n",
    "---------------------------------------------------------------------\n",
    "\n",
    "This is my custom code for combining all the CVAT annotation files (_Format: COCO v1_) and splitting them into **Train**, **Test** and **Validation** .json files 😎. \n",
    "\n",
    "**!! Important thing to note while importing annotations from CVAT:**\n",
    "When we annotate in CVAT, for each clip we receive one annotation file. Unfortunately, these annotation files have _'frame numbers'_ that start only from 0 but Detectron 2 needs it to start from **1**. In the below code cell, we are taking care of this by using the function _adjustFrameDifference()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsrvMpH46V0_",
    "outputId": "31f43860-d23b-4112-9583-0f3c1a7a7eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Processing the following annotation files: \n",
      "  - D:\\Project_Escooter_Tracking\\input\\16\\16_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\18\\18_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\2\\2_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\20\\20_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\21\\21_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\22\\22_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\23\\23_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\24\\24_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\25\\25_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\26\\26_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\27\\27_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\28\\28_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\29\\29_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\3\\3_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\30\\30_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\31\\31_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\32\\32_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\33\\33_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\34\\34_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\35\\35_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\36\\36_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\37\\37_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\38\\38_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\39\\39_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\4\\4_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\40\\40_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\41\\41_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\42\\42_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\43\\43_Annotations.json\n",
      "  - D:\\Project_Escooter_Tracking\\input\\44\\44_Annotations.json\n",
      "\n",
      "- Total no.of annotations/images in the dataset: 22605\n",
      "\n",
      "- Splitting the dataset into Train, Valid and Test is successfull\n",
      "\n",
      "- Saving train, test and valid annotation files\n",
      "  - Final training set file saved as: D:\\Project_Escooter_Tracking\\input\\Test_1_Train.json\n",
      "  - Final valid set file saved as: D:\\Project_Escooter_Tracking\\input\\Test_1_Valid.json\n",
      "  - Final test set file saved as: D:\\Project_Escooter_Tracking\\input\\Test_1_Test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# coco_format is the dict file which includes all the values that needs to be output in the final annotations json file\n",
    "# Some of the key values like 'licenses', 'info' and 'categories' are constant and declared at first here\n",
    "\n",
    "coco_format = {\n",
    "    \"licenses\": [{\n",
    "        \"name\": \"\",\n",
    "        \"id\": 0,\n",
    "        \"url\": \"\"\n",
    "    }],\n",
    "    \"info\": {\n",
    "        \"contributor\": \"Vishal Balaji\",\n",
    "        \"date_created\": \"\",\n",
    "        \"description\": \"Escooter Dataset\",\n",
    "        \"url\": \"\",\n",
    "        \"version\": \"\",\n",
    "        \"year\": \"\"\n",
    "    },\n",
    "    \"categories\": [{\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Escooter\",\n",
    "        \"supercategory\": \"\"\n",
    "    }]\n",
    "}\n",
    "\n",
    "# The key values 'images' and 'annotations' needs to be processed and appended. The below given lines is the format for\n",
    "# those dicts.\n",
    "\"\"\"\n",
    "\"images\":[\n",
    "    {\n",
    "        \"id\":1,\n",
    "        \"width\": 1920,\n",
    "        \"height\": 1080,\n",
    "        \"file_name\":\"sdfa.PNG\",\n",
    "        \"license\":0,\n",
    "        \"flickr_url\": \"\",\n",
    "        \"coco_url\": \"\",\n",
    "        \"date_captured\": 0\n",
    "    }\n",
    "]\n",
    "\n",
    "\"annotations\":[\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"image_id\": 55,\n",
    "        \"category_id\": 1,\n",
    "        \"segmentation\": [[]],\n",
    "        \"area\": {some area number in float},\n",
    "        \"bbox\": [].\n",
    "        \"iscrowd\": 0\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Path where the annotations are stored, when the repo is the path of current working directory\n",
    "#main_file_path = os.path.abspath(r'D:\\Carissma Video Copy\\Traffic Camera Tracking\\Finished')\n",
    "input_path = r'D:\\Project_Escooter_Tracking\\input'\n",
    "main_file_path = input_path\n",
    "\n",
    "# Declaration of empty lists that is later appended it with images and annotations.\n",
    "images_list = []\n",
    "annotations_list = []\n",
    "\n",
    "# Each image and annotations has an ID associated with it and it starts with 1.\n",
    "# These values are incremented as the images and annotations are being added.\n",
    "img_num = 1\n",
    "anno_num = 1\n",
    "\n",
    "def adjustFrameDifference(file_name, offset=1):\n",
    "    # Adjusting for difference in frame\n",
    "    file_name_from_dict = file_name.split('.')[0]\n",
    "    file_number = int(file_name_from_dict[-6:])\n",
    "    \n",
    "    # 1 is the offset number for the frame difference between the annotations \n",
    "    # from CVAT and frames extracted from the FFMPEG Script\n",
    "    file_number += 1\n",
    "    \n",
    "    # Adding the write number of 0's and taking care of proper filename\n",
    "    if int(file_number / 10) == 0:\n",
    "      new_file_name = file_name_from_dict[:-6] + '00000' + str(file_number) + '.png'\n",
    "    elif int(file_number / 100) == 0:\n",
    "      new_file_name = file_name_from_dict[:-6] + '0000' + str(file_number) + '.png'\n",
    "    elif int(file_number / 1000) == 0:\n",
    "      new_file_name = file_name_from_dict[:-6] + '000' + str(file_number) + '.png'\n",
    "    elif int(file_number / 10000) == 0:\n",
    "      new_file_name = file_name_from_dict[:-6] + '00' + str(file_number) + '.png'\n",
    "    \n",
    "    return new_file_name\n",
    "\n",
    "\n",
    "print(\"- Processing the following annotation files: \")\n",
    "for clip_number, clips in enumerate(os.listdir(main_file_path)):\n",
    "    # Checking that only numbers are given as folder names for the clips\n",
    "    if all(char.isdigit() for char in clips):\n",
    "      # Path of the clips folder\n",
    "      clips_path = main_file_path + '\\\\' + clips\n",
    "      # Path of the annotation of the clips\n",
    "      annotation_file = clips_path + f'\\\\{str(clips)}_Annotations.json'\n",
    "\n",
    "      file = open(annotation_file)\n",
    "      json_file = json.load(file)\n",
    "      print(f'  - {annotation_file}')\n",
    "        \n",
    "      \n",
    "      # !! Testing purpose only for restricting number of annotations\n",
    "      # flag = 1\n",
    "      for annotations in json_file['annotations']:\n",
    "\n",
    "          anno_image_ID = annotations['image_id']\n",
    "          anno_ID = annotations['id']\n",
    "\n",
    "          image_filename = ''\n",
    "          for images in json_file['images']:\n",
    "              if images['id'] == anno_image_ID:\n",
    "                  image_filename = images['file_name']\n",
    "\n",
    "          filename = input_path + '\\\\' + clips + '\\\\images\\\\' + image_filename\n",
    "          filename = adjustFrameDifference(filename)  \n",
    "          \n",
    "        # The formats for 'images' dictionary and 'annotations' dictionary in COCO\n",
    "          image_dict = {\n",
    "              'id': img_num,\n",
    "              \"width\": 1920,\n",
    "              \"height\": 1080,\n",
    "              \"file_name\": filename,\n",
    "              \"license\": 0,\n",
    "              \"flickr_url\": \"\",\n",
    "              \"coco_url\": \"\",\n",
    "              \"date_captured\": 0\n",
    "          }\n",
    "          anno_dict = {\n",
    "              \"id\": anno_num,\n",
    "              'image_id': img_num,\n",
    "              \"category_id\": 1,\n",
    "              'segmentation': annotations['segmentation'],\n",
    "              'area': annotations['area'],\n",
    "              'bbox': annotations['bbox'],\n",
    "              'iscrowd': annotations['iscrowd']\n",
    "          }\n",
    "\n",
    "          # In the COCO-Format, every images and associated annotations are passed as array of dicts.\n",
    "          images_list.append(image_dict)\n",
    "          annotations_list.append(anno_dict)\n",
    "\n",
    "          # Incrementing the Image ID and Annotation ID for each loop\n",
    "          img_num += 1\n",
    "          anno_num += 1\n",
    "        \n",
    "      file.close()\n",
    "\n",
    "      # !! Meant for testing purpose.\n",
    "      # if clip_number == 1:\n",
    "      #     break\n",
    "\n",
    "print(f'\\n- Total no.of annotations/images in the dataset: {anno_num}')\n",
    "\n",
    "train_json = deepcopy(coco_format)\n",
    "valid_json = deepcopy(coco_format)\n",
    "test_json = deepcopy(coco_format)\n",
    "\n",
    "train_split = 0.8\n",
    "valid_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "# Function to split the whole dataset of images and annotations into train,\n",
    "# valid and test sets\n",
    "def splitDataset(images, annotations, trainSplit, validSplit):\n",
    "  trainSize = int(len(images) * trainSplit)\n",
    "  train_images = []\n",
    "  train_annotations = []\n",
    "  \n",
    "  copy_images = list(images)\n",
    "  copy_annotations = list(annotations)\n",
    "  while len(train_images) < trainSize:\n",
    "    index = random.randrange(len(copy_images))\n",
    "    train_images.append(copy_images.pop(index))\n",
    "    train_annotations.append(copy_annotations.pop(index))\n",
    "  \n",
    "\n",
    "  copySize = int(len(copy_images) * (validSplit/(1 - trainSplit)))\n",
    "  valid_images = []\n",
    "  valid_annotations = []\n",
    "\n",
    "  test_images = copy_images\n",
    "  test_annotations = copy_annotations\n",
    "  while len(valid_images) < copySize:\n",
    "    index = random.randrange(len(test_images))\n",
    "    valid_images.append(test_images.pop(index))\n",
    "    valid_annotations.append(test_annotations.pop(index))\n",
    "  \n",
    "  return [(train_images, train_annotations), (valid_images, valid_annotations), (test_images, test_annotations)]\n",
    "\n",
    "train_set, valid_set, test_set = splitDataset(images_list, annotations_list, 0.8, 0.1)\n",
    "print(\"\\n- Splitting the dataset into Train, Valid and Test is successfull\\n\")\n",
    "\n",
    "# Storing the processed arrays of images and annotations with their\n",
    "# respective keys in the final dataset\n",
    "# coco_format[\"images\"] = images_list\n",
    "# coco_format[\"annotations\"] = annotations_list\n",
    "\n",
    "train_json['images'] = train_set[0]\n",
    "train_json['annotations'] = train_set[1]\n",
    "\n",
    "valid_json['images'] = valid_set[0]\n",
    "valid_json['annotations'] = valid_set[1]\n",
    "\n",
    "test_json['images'] = test_set[0]\n",
    "test_json['annotations'] = test_set[1]\n",
    "\n",
    "# Code Snippet to automatically create new names for the many\n",
    "# .json files created during the testing\n",
    "base_filename = 'Test_'\n",
    "for numbers in range(20):\n",
    "    check_filename = base_filename + str(numbers+1) + '.json'\n",
    "    if check_filename not in os.listdir(os.getcwd()):\n",
    "        base_filename = check_filename\n",
    "        break\n",
    "\n",
    "\n",
    "# These lines writes all the dictionaries into the final required .json file\n",
    "# For train, valid and test individually\n",
    "train_file = f\"{input_path}\\\\{base_filename[:-5]}_Train.json\"\n",
    "valid_file = f\"{input_path}\\\\{base_filename[:-5]}_Valid.json\"\n",
    "test_file = f\"{input_path}\\\\{base_filename[:-5]}_Test.json\"\n",
    "\n",
    "print(\"- Saving train, test and valid annotation files\")\n",
    "with open(train_file, \"w\") as file:\n",
    "    json.dump(train_json, file)\n",
    "    print(f\"  - Final training set file saved as: {train_file}\")\n",
    "\n",
    "with open(valid_file, \"w\") as file:\n",
    "    json.dump(valid_json, file)\n",
    "    print(f\"  - Final valid set file saved as: {valid_file}\")\n",
    "\n",
    "with open(test_file, \"w\") as file:\n",
    "    json.dump(test_json, file)\n",
    "    print(f\"  - Final test set file saved as: {test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the annotations\n",
    "\n",
    "We have loaded all our annotations in the right format from .json files. To check whether the annotations are loaded correctly and mapped to the right images, we are randomly selecting _8_ images and displaying the annotations on top of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7TF5g6O0HCWh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/20 07:13:51 d2.data.datasets.coco]: \u001b[0mLoaded 2261 images in COCO format from D:\\Project_Escooter_Tracking\\input\\Test_1_Test.json\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Registering the datasets to the Detectron2 model. Using try-except block because dataset can be registered only once.\n",
    "# If we need to run this code block multiple times, then we need to ignore the 'AssertionError'\n",
    "try:\n",
    "    register_coco_instances(\"escooter_train\", {}, train_file, '')\n",
    "except AssertionError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    register_coco_instances(\"escooter_valid\", {}, valid_file, '')\n",
    "except AssertionError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    register_coco_instances(\"escooter_test\", {}, test_file, '')\n",
    "except AssertionError:\n",
    "    pass\n",
    "\n",
    "dataset_dicts = DatasetCatalog.get(\"escooter_test\")\n",
    "\n",
    "# List of labelled_images for visualization purposes\n",
    "out_files = []\n",
    "\n",
    "# Making a test_labels folder\n",
    "sample_label_path = f'{input_path}\\sample_labels'\n",
    "os.system(f'mkdir {sample_label_path}')\n",
    "\n",
    "# Just selecting 8 random images for visualizing purposes\n",
    "for d in random.sample(dataset_dicts, 8):   \n",
    "    \n",
    "    img = cv2.imread(d['file_name'])\n",
    "\n",
    "    visualizer = Visualizer(img[:, :, ::-1], scale=1)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "\n",
    "    out_filename = sample_label_path + '\\\\' + d['file_name'][:-4].split('\\\\')[-1] + '.png'\n",
    "    cv2.imwrite(out_filename, out.get_image()[:, :, ::-1])\n",
    "    out_files.append(out_filename)\n",
    "\n",
    "# Displays images in notebook. Caution: 'display()' may not work outside this notebook\n",
    "for images in out_files:\n",
    "  display(Image(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4lz0o5-chBi"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "This Config _cfg_ variable is very important in Detectron2 and must be tuned perfectly for optimal results. So far, ideal values have been copied from internet and used but **HyperParameter Tuning** is yet to be done.\n",
    "For our purpose, we are using transfer learning here to use an already COCO-pretrained R50-FPN Mask R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eR2KTj5JcfGS",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/20 07:11:15 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Dataset 'escooter_train' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_minival_100, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2014_minival_100, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\data\\catalog.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Traffic_Camera_Tracking\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1057\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1058\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'escooter_train'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-417eba91fbaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#trainer.train()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\engine\\defaults.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cfg)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0mdata_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_train_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;31m# For training, wrap with DDP. But don't need this for inference.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\engine\\defaults.py\u001b[0m in \u001b[0;36mbuild_train_loader\u001b[1;34m(cls, cfg)\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mOverwrite\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0myou\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0md\u001b[0m \u001b[0mlike\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \"\"\"\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbuild_detection_train_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\config\\config.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m_called_with_cfg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                     \u001b[0mexplicit_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_args_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0morig_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mexplicit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\config\\config.py\u001b[0m in \u001b[0;36m_get_args_from_config\u001b[1;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msupported_arg_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[0mextra_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_config_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[1;31m# forward the other arguments to __init__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\data\\build.py\u001b[0m in \u001b[0;36m_train_loader_from_config\u001b[1;34m(cfg, mapper, dataset, sampler)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_train_loader_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m         dataset = get_detection_dataset_dicts(\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mfilter_empty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDATALOADER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFILTER_EMPTY_ANNOTATIONS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\data\\build.py\u001b[0m in \u001b[0;36mget_detection_dataset_dicts\u001b[1;34m(names, filter_empty, min_keypoints, proposal_files)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m     \u001b[0mdataset_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDatasetCatalog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdicts\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_dicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Dataset '{}' is empty!\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\data\\build.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m     \u001b[0mdataset_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDatasetCatalog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdicts\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_dicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Dataset '{}' is empty!\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\balaji\\desktop\\traffic_camera_tracking\\detectron2_setup\\detectron2\\data\\catalog.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             raise KeyError(\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \"Dataset '{}' is not registered! Available datasets are: {}\".format(\n\u001b[0;32m     55\u001b[0m                     \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Dataset 'escooter_train' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_minival_100, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2014_minival_100, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val\""
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"escooter_train\",)\n",
    "cfg.DATASETS.TEST = ('escooter_test',)\n",
    "cfg.DATALOADER.NUM_WORKERS = 42\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.WARMUP_ITERS = 100\n",
    "cfg.SOLVER.MAX_ITER = 5000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1yDEcrn3rXki"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5784), started 0:58:33 ago. (Use '!kill 5784' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f200b405a06c7dee\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f200b405a06c7dee\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looking at the values of loss and other parameters in the tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now using the trained model to check the validation set (Not the same training data for obvious reasons😅)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# load weights\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "# Set training data-set path\n",
    "cfg.DATASETS.TEST = (\"escooter_valid\", )\n",
    "# Create predictor (model for inference)\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "valid_images = []\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=escooter_metadata, \n",
    "                   scale=0.8, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "   \n",
    "    validation_label_path = f'{input_path}\\\\valid_labels'\n",
    "    os.system(f'mkdir {validation_label_path}')\n",
    "\n",
    "    out_filename = validation_label_path + '\\\\' + d['file_name'][:-4].split('\\\\')[-1] + '.png'\n",
    "    cv2.imwrite(out_filename, v.get_image()[:, :, ::-1])\n",
    "    valid_images.append(out_filename)\n",
    "\n",
    "# Displays images in notebook. Caution: 'display()' may not work outside this notebook\n",
    "for images in valid_images:\n",
    "  display(Image(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on images and Videos\n",
    "\n",
    "With this code cell, we can test the model on individual images or videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/20 07:38:47 d2.data.datasets.coco]: \u001b[0mLoaded 2260 images in COCO format from D:\\Project_Escooter_Tracking\\input\\Test_1_Valid.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-401a6e88ee93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;31m#inference_video(r'D:\\Project_Escooter_Tracking\\test\\images')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Displays images in notebook. Caution: 'display()' may not work outside this notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'size' is not defined"
     ]
    }
   ],
   "source": [
    "# Inference on normal images\n",
    "\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# load weights\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "\n",
    "# Create predictor (model for inference)\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "dataset_dicts = DatasetCatalog.get(\"escooter_valid\")\n",
    "escooter_metadata = MetadataCatalog.get(\"escooter_valid\")\n",
    "\n",
    "input_path = r'D:\\Project_Escooter_Tracking\\input'\n",
    "path_images = r'D:\\Project_Escooter_Tracking\\test\\images'\n",
    "existing_files = []\n",
    "for img in os.listdir(path_images):\n",
    "  fileName = path_images + '\\\\' + img\n",
    "  existing_files.append(fileName)\n",
    "\n",
    "final_test_label_path = f'{input_path}\\\\test_inference_video'\n",
    "os.system(f'mkdir {final_test_label_path}')\n",
    "\n",
    "def inference_single_image(filename, output):\n",
    "    im = cv2.imread(filename)\n",
    "    #im = frame\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=escooter_metadata, \n",
    "                   scale=0.8, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "   \n",
    "    out_filename = output + '\\\\' + filename[:-4].split('\\\\')[-1] + '.png'\n",
    "    cv2.imwrite(out_filename, v.get_image()[:, :, ::-1])\n",
    "    return out_filename\n",
    "\n",
    "labeled_images = []\n",
    "for d in existing_files:    \n",
    "    labeled_images.append(inference_single_image(d, final_test_label_path))\n",
    "    \n",
    "def inference_video(filepath):\n",
    "#     video = cv2.VideoCapture(filepath)\n",
    "#     while video.isOpened():\n",
    "#             success, frame = video.read()\n",
    "#             if success:\n",
    "#                 inference_single_image(frame, final_test_label_path)\n",
    "#             else:\n",
    "#                 break\n",
    "    pass\n",
    "\n",
    "print(size(labeled_images))\n",
    "#inference_video(r'D:\\Project_Escooter_Tracking\\test\\images')\n",
    "# Displays images in notebook. Caution: 'display()' may not work outside this notebook\n",
    "#for images in labeled_images:\n",
    "  #display(Image(images))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Escooter Tracking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
