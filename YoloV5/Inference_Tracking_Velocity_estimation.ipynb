{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference_Tracking_Velocity_estimation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SODpI5ulsgFL"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FmYW-qIvcOL",
        "outputId": "91ee4393-fb3b-4a0a-dcfb-47c6bcb78320"
      },
      "source": [
        "!git clone https://github.com/savenow/Traffic_Camera_Tracking.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Traffic_Camera_Tracking'...\n",
            "remote: Enumerating objects: 13765, done.\u001b[K\n",
            "remote: Counting objects: 100% (13564/13564), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8287/8287), done.\u001b[K\n",
            "remote: Total 13765 (delta 909), reused 13470 (delta 821), pack-reused 201\u001b[K\n",
            "Receiving objects: 100% (13765/13765), 243.36 MiB | 26.58 MiB/s, done.\n",
            "Resolving deltas: 100% (990/990), done.\n",
            "Checking out files: 100% (18872/18872), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyUWDa40v6AZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41997c7-6f90-4603-e4b5-f244ef69630f"
      },
      "source": [
        "!pip install tqdm albumentations filterpy scikit-image lap pyyaml"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (0.18.3)\n",
            "Collecting lap\n",
            "  Downloading lap-0.4.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
            "\u001b[K     |████████████████████████████████| 631 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.2.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (7.1.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2021.11.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
            "Building wheels for collected packages: imgaug, filterpy, lap\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654020 sha256=a8d00bef854bd71779a6c0df4613b251b4cc719bcb0f97683c1019bfc60e7149\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/72/98/3ebfdba1069a9a8eaaa7ae7265cfd67d63ef0197aaee2e5f9c\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=89e08193c085c35a23ad57d952fbf57c0b19b39dbabd13b498dfe17c09b85398\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lap: filename=lap-0.4.0-cp37-cp37m-linux_x86_64.whl size=1590157 sha256=e7dfa31eb4e52092dc0eb724b94aa02bfafe5211033c0c5543d88973a4786b3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/0b/e3/ef9daf1b5547b56389e42c80c3100f1e6479bf5fd00fd9d6ba\n",
            "Successfully built imgaug filterpy lap\n",
            "Installing collected packages: imgaug, lap, filterpy\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed filterpy-1.4.5 imgaug-0.2.6 lap-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5auEr7ePza1a",
        "outputId": "4984616f-cced-42ed-82d7-e22e030ef713"
      },
      "source": [
        "# download input video file and weight file\n",
        "!gdown --id 1EqndRhq7FZE4C4y-h7Y-KX3btGt0gFLr\n",
        "!gdown --id 1vS6YN36mBxrpqug7OPAAoIAXMqnjHyOC\n",
        "!gdown --id 1TgDjntHh3THnpMvnGDL99kZHHLocHgVh\n",
        "!gdown --id 1LilfAbWTEq8z40Tkjm_Kq_2CuWJ0T1UY"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EqndRhq7FZE4C4y-h7Y-KX3btGt0gFLr\n",
            "To: /content/tl_extended_epoch_30.pt\n",
            "100% 154M/154M [00:01<00:00, 154MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vS6YN36mBxrpqug7OPAAoIAXMqnjHyOC\n",
            "To: /content/31.mp4\n",
            "100% 11.1M/11.1M [00:00<00:00, 42.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TgDjntHh3THnpMvnGDL99kZHHLocHgVh\n",
            "To: /content/tl_yolo5l6_78k_bs_3.pt\n",
            "100% 154M/154M [00:00<00:00, 159MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LilfAbWTEq8z40Tkjm_Kq_2CuWJ0T1UY\n",
            "To: /content/tl_89k_bs24_im1408_e50.pt\n",
            "100% 153M/153M [00:01<00:00, 84.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXGUgOEPu3xL"
      },
      "source": [
        "## Tracking Code below..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bBdNT918RWj"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TKAgg')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjbzFwNt3F2S"
      },
      "source": [
        "import importlib\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#from Traffic_Camera_Tracking.YoloV5.yolo_v5_main_files.utils.torch_utils import time_sync\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch.backends.cudnn as cudnn\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# # Importing sort\n",
        "from Traffic_Camera_Tracking.YoloV5.sort_yolo import Sort\n",
        "from Traffic_Camera_Tracking.YoloV5.sort_yolo import KalmanBoxTracker\n",
        "# importlib.reload(Sort)\n",
        "\n",
        "def show_tracker_bbox(input, frame, Score_ClassIDs):\n",
        "    classID_dict = {0: (\"Escooter\", (0, 90, 255)), 1: (\"Pedestrians\", (255, 90, 0)), 2: (\"Cyclists\", (90, 255, 0))}\n",
        "    \n",
        "    img = frame\n",
        "    for detection, Score_ClassID in zip(input, Score_ClassIDs):\n",
        "        score = Score_ClassID[0]\n",
        "        classID = int(Score_ClassID[1])\n",
        "        color = classID_dict[classID][1]\n",
        "\n",
        "        # For bounding box\n",
        "        #print(f\"Detection: {detection}\")\n",
        "        tracker_id = int(detection[4])\n",
        "        x1, y1, x2, y2 = detection[0:4]\n",
        "        x1 = int(x1)\n",
        "        x2 = int(x2)\n",
        "        y1 = int(y1)\n",
        "        y2 = int(y2)\n",
        "\n",
        "        #print(f\"Tracker ID: {tracker_id}, classID:{classID}\")\n",
        "        #print(f\"X1: {x1}, Y1: {y1}, X2: {x2}, Y2: {y2}\")\n",
        "\n",
        "        img = cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "        \n",
        "        # For the text background\n",
        "        # Finds space required by the text so that we can put a background with that amount of width.\n",
        "        label_1 = f'Track_ID: {tracker_id}'\n",
        "        (w1, h1), _ = cv2.getTextSize(\n",
        "                label_1, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "        \n",
        "        label_2 = f'{classID_dict[classID][0]} {round(score*100, 1)}%'\n",
        "        (w2, h2), _ = cv2.getTextSize(\n",
        "                label_2, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "\n",
        "        # Prints the text.    \n",
        "        img = cv2.rectangle(img, (x1, y1 - 40), (x1 + w1, y1), color, -1)\n",
        "        img = cv2.rectangle(img, (x1, y1 - 20), (x1 + w2, y1), color, -1)\n",
        "        text_color = (0, 0, 0)\n",
        "        img = cv2.putText(img, label_1, (x1, y1 - 24),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1)\n",
        "        img = cv2.putText(img, label_2, (x1, y1 - 5),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1)\n",
        "        del color\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_tracker_bbox_2(input, frame):\n",
        "    classID_dict = {0: (\"Escooter\", (0, 90, 255)), 1: (\"Pedestrians\", (255, 90, 0)), 2: (\"Cyclists\", (90, 255, 0))}\n",
        "    #print(input)\n",
        "    img = frame\n",
        "    for detection in input: # dtection = tracker in inference function\n",
        "        # For bounding box\n",
        "        # print(f\"Detection: {detection}\")\n",
        "        tracker_id = int(detection[9])\n",
        "        x1, y1, x2, y2 = detection[0:4]\n",
        "        x1 = int(x1)\n",
        "        x2 = int(x2)\n",
        "        y1 = int(y1)\n",
        "        y2 = int(y2)\n",
        "        score = detection[4]\n",
        "        classID = detection[5]\n",
        "\n",
        "        #print(f\"Tracker ID: {tracker_id}, classID:{classID}\")\n",
        "        #print(f\"X1: {x1}, Y1: {y1}, X2: {x2}, Y2: {y2}\")\n",
        "\n",
        "        color = classID_dict[classID][1]\n",
        "        img = cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "        \n",
        "        # For the text background\n",
        "        # Finds space required by the text so that we can put a background with that amount of width.\n",
        "        label_1 = f'Track_ID: {tracker_id}'\n",
        "        (w1, h1), _ = cv2.getTextSize(\n",
        "                label_1, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "        \n",
        "        label_2 = f'{classID_dict[classID][0]} {round(score*100, 1)}%'\n",
        "        (w2, h2), _ = cv2.getTextSize(\n",
        "                label_2, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "\n",
        "        # Prints the text.    \n",
        "        img = cv2.rectangle(img, (x1, y1 - 40), (x1 + w1, y1), color, -1)\n",
        "        img = cv2.rectangle(img, (x1, y1 - 20), (x1 + w2, y1), color, -1)\n",
        "        text_color = (0, 0, 0)\n",
        "        img = cv2.putText(img, label_1, (x1, y1 - 24),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1)\n",
        "        img = cv2.putText(img, label_2, (x1, y1 - 5),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1)\n",
        "        del color\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "class TqdmExtraFormat(tqdm):\n",
        "    \"\"\"Provides a `total_time` format parameter\"\"\"\n",
        "    @property\n",
        "    def format_dict(self):\n",
        "        d = super(TqdmExtraFormat, self).format_dict\n",
        "        total_time = d[\"elapsed\"] * (d[\"total\"] or 0) / max(d[\"n\"], 1)\n",
        "        d.update(total_time=self.format_interval(total_time) + \" in total\")\n",
        "        return d\n",
        "\n",
        "\n",
        "def video_inference(input_path, output_path, model_weights, class_name=None):\n",
        "    video_capture = cv2.VideoCapture(input_path)\n",
        "    width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    video_duration = total_frames / fps\n",
        "    codec = 'mp4v'\n",
        "\n",
        "    video_output = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*codec), float(fps), (width, height),)\n",
        "    model = torch.hub.load(r'/content/Traffic_Camera_Tracking/YoloV5/yolo_v5_main_files',\n",
        "                           'custom',\n",
        "                           path=model_weights,\n",
        "                           source='local')\n",
        "    framecount = 1\n",
        "\n",
        "    device = torch.device('cuda:0')\n",
        "    cudnn.benchmark = True\n",
        "   \n",
        "    #del Object_tracker\n",
        "    Object_tracker = Sort(max_age=30, min_hits=7, iou_threshold=0.15)\n",
        "    Object_tracker.reset_count()\n",
        "\n",
        "    # pred = KalmanBoxTracker.predict()\n",
        "\n",
        "    pbar = TqdmExtraFormat(total = total_frames, desc='Inference Progress: ')\n",
        "    while video_capture.isOpened():\n",
        "        _, frame = video_capture.read()\n",
        "        \n",
        "        if _:\n",
        "            # Results. Try printing or modifying this array to get different classes\n",
        "            results = model(frame, size=1920)\n",
        "            result = results.xyxy[0]\n",
        "            \n",
        "            score_class_id = []\n",
        "            if len(result) > 0: \n",
        "              dets = []\n",
        "              for items in result:\n",
        "                # items[:5].detach().cpu().numpy()\n",
        "                # print(items)\n",
        "                dets.append(items[:].tolist())\n",
        "                #score_class_id.append((items[4].item(), items[5].item()))\n",
        "              \n",
        "              dets = np.array(dets)\n",
        "              # print('dets : ', dets)\n",
        "              tracker = Object_tracker.update(dets)\n",
        "              \n",
        "            else:\n",
        "              tracker = Object_tracker.update()\n",
        "            # print('tracker : ',tracker)\n",
        "            \n",
        "           \n",
        "            video_output.write(show_tracker_bbox_2(tracker, frame))\n",
        "            pbar.update(framecount)\n",
        "\n",
        "            \n",
        "        else:\n",
        "          break\n",
        "\n",
        "    video_capture.release()\n",
        "    video_output.release()\n",
        "\n",
        "# Model\n",
        "model_weight = r'/content/tl_yolo5l6_78k_bs_3.pt'\n",
        "input_directory = r'/content/31.mp4'\n",
        "output_directory = r'/content/3_pt_result.mp4'\n",
        "class_name = None\n",
        "video_inference(input_directory, output_directory, model_weight, class_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdMYFQXbuqTt"
      },
      "source": [
        "## Edited above code for velocity Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdoK1sF4_b6K"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('TKAgg')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAoBb3HxX02o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ea6d8e-8389-4030-8fca-c629201ebea3"
      },
      "source": [
        "import importlib\n",
        "import sys\n",
        "import math\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#from Traffic_Camera_Tracking.YoloV5.yolo_v5_main_files.utils.torch_utils import time_sync\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import torch.backends.cudnn as cudnn\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Importing sort\n",
        "from Traffic_Camera_Tracking.YoloV5.sort_yolo import Sort\n",
        "from Traffic_Camera_Tracking.YoloV5.sort_yolo import KalmanBoxTracker\n",
        "# importlib.reload(Sort)\n",
        "\n",
        "\"\"\"\n",
        "values for drawing the boundaries in frames...\n",
        "# line_1 = [(836, 121), (813, 209)]\n",
        "# line_1 = [(1277, 125), (1276, 216)]\n",
        "# rec = [(36,59), (1521, 59), (1521,1003), (36, 1003)]\n",
        "\"\"\"\n",
        "\n",
        "def show_tracker_bbox_2(input, frame, track_dict, track_count):\n",
        "    classID_dict = {0: (\"Escooter\", (0, 90, 255)), 1: (\"Pedestrians\", (255, 90, 0)), 2: (\"Cyclists\", (90, 255, 0))}\n",
        "    img = frame\n",
        "    for detection in input: # dtection = tracker in inference function\n",
        "        tracker_id = int(detection[9])\n",
        "        x1, y1, x2, y2 = detection[0:4]\n",
        "        x1 = int(x1)\n",
        "        x2 = int(x2)\n",
        "        y1 = int(y1)\n",
        "        y2 = int(y2)\n",
        "        score = detection[4]\n",
        "        classID = detection[5]\n",
        "        cx = int((x1 + x2)/2) # center_x of bbox\n",
        "        cy = int((y1 +y2)/2)  # center_y of bbox\n",
        "        track_dict[tracker_id].append((cx,cy))\n",
        "        track_count += 1\n",
        "        \n",
        "        color = classID_dict[classID][1]\n",
        "        \n",
        "        img = cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "        img = cv2.circle(img, (cx, cy), 4, color, -1)\n",
        "        \n",
        "        # For the text background\n",
        "        # Finds space required by the text so that we can put a background with that amount of width.\n",
        "        label_1 = f'Track_ID: {tracker_id}'\n",
        "        (w1, h1), _ = cv2.getTextSize(\n",
        "                label_1, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "        \n",
        "        label_2 = f'{classID_dict[classID][0]} {round(score*100, 1)}%'\n",
        "        (w2, h2), _ = cv2.getTextSize(\n",
        "                label_2, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "\n",
        "        # Prints the text.    \n",
        "        img = cv2.rectangle(img, (x1, y1 - 40), (x1 + w1, y1), color, -1)\n",
        "        img = cv2.rectangle(img, (x1, y1 - 20), (x1 + w2, y1), color, -1)\n",
        "        text_color = (0, 0, 0)\n",
        "        img = cv2.putText(img, label_1, (x1, y1 - 24),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1)\n",
        "        img = cv2.putText(img, label_2, (x1, y1 - 5),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1)\n",
        "        del color\n",
        "        \n",
        "        \n",
        "    return img, track_dict, track_count\n",
        "\n",
        "\n",
        "\n",
        "class TqdmExtraFormat(tqdm):\n",
        "    \"\"\"Provides a `total_time` format parameter\"\"\"\n",
        "    @property\n",
        "    def format_dict(self):\n",
        "        d = super(TqdmExtraFormat, self).format_dict\n",
        "        total_time = d[\"elapsed\"] * (d[\"total\"] or 0) / max(d[\"n\"], 1)\n",
        "        d.update(total_time=self.format_interval(total_time) + \" in total\")\n",
        "        return d\n",
        "\n",
        "\n",
        "def video_inference(input_path, output_path, model_weights, class_name=None):\n",
        "    video_capture = cv2.VideoCapture(input_path)\n",
        "    width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    video_duration = total_frames / fps\n",
        "    codec = 'mp4v'\n",
        "\n",
        "    video_output = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*codec), float(fps), (width, height),)\n",
        "    model = torch.hub.load(r'/content/Traffic_Camera_Tracking/YoloV5/yolo_v5_main_files',\n",
        "                           'custom',\n",
        "                           path=model_weights,\n",
        "                           source='local')\n",
        "    framecount = 1\n",
        "\n",
        "    device = torch.device('cuda:0')\n",
        "    cudnn.benchmark = True\n",
        "   \n",
        "    #del Object_tracker\n",
        "    Object_tracker = Sort(max_age=30, min_hits=7, iou_threshold=0.15)\n",
        "    Object_tracker.reset_count()\n",
        "\n",
        "    # pred = KalmanBoxTracker.predict()\n",
        "\n",
        "    # Tracking vehicles:\n",
        "    velocity_frame_window = 5\n",
        "    FPS_OF_RECORDING = 30.\n",
        "    track_dict = defaultdict(list)\n",
        "    track_count = 0\n",
        "    count = 0\n",
        "    classID_dict = {0: (\"Escooter\", (0, 90, 255)), 1: (\"Pedestrians\", (255, 90, 0)), 2: (\"Cyclists\", (90, 255, 0))}\n",
        "    pbar = TqdmExtraFormat(total = total_frames, desc='Inference Progress: ')\n",
        "    while video_capture.isOpened():\n",
        "        _, frame = video_capture.read()\n",
        "        count += 1\n",
        "\n",
        "        # if count%8 != 0:\n",
        "        #   continue\n",
        "        \n",
        "        if _:\n",
        "            # Results. Try printing or modifying this array to get different classes\n",
        "            results = model(frame, size=1920)\n",
        "            result = results.xyxy[0]\n",
        "            \n",
        "            # score_class_id = []\n",
        "            if len(result) > 0: \n",
        "              dets = []\n",
        "              for items in result:\n",
        "                dets.append(items[:].tolist())\n",
        "              \n",
        "              dets = np.array(dets)\n",
        "              # print('dets : ', dets)\n",
        "              tracker = Object_tracker.update(dets)\n",
        "              \n",
        "            else:\n",
        "              tracker = Object_tracker.update()\n",
        "            # print('tracker : ',tracker)\n",
        "\n",
        "            img, track_dict, track_count = show_tracker_bbox_2(tracker, frame, track_dict, track_count)\n",
        "            \n",
        "            if velocity_frame_window < track_count:\n",
        "              for i in track_dict.keys():\n",
        "                if len(track_dict[i])>velocity_frame_window:\n",
        "                  # prev_point = track_dict[i][track_count - 1 -velocity_frame_window] #  <-- this will give an error becz of deleting values from dict \n",
        "                  prev_point = track_dict[i][0]\n",
        "                  curr_point = track_dict[i][-1]\n",
        "                  del track_dict[i][0]\n",
        "                  distance = int(math.sqrt(math.pow(prev_point[0] - curr_point[0], 2) + math.pow(prev_point[1] - curr_point[1], 2)))\n",
        "                  velo_in_pix_s = int((distance * FPS_OF_RECORDING)/ (velocity_frame_window)) \n",
        "                  s_label = f'speed: {int(velo_in_pix_s)} pix/s'\n",
        "                  cv2.putText(img, s_label, (int(curr_point[0])+15, int(curr_point[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2)\n",
        "\n",
        "            video_output.write(img)\n",
        "            pbar.update(framecount)\n",
        "            \n",
        "        else:\n",
        "          break\n",
        "\n",
        "    video_capture.release()\n",
        "    video_output.release()\n",
        "\n",
        "# Model\n",
        "model_weight = r'/content/tl_yolo5l6_78k_bs_3.pt'\n",
        "input_directory = r'/content/31.mp4'\n",
        "output_directory = r'/content/vel_result_2.mp4'\n",
        "class_name = None\n",
        "video_inference(input_directory, output_directory, model_weight, class_name)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 3e24bc1 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Installing collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /content/Traffic_Camera_Tracking/YoloV5/yolo_v5_main_files/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 476 layers, 76134048 parameters, 0 gradients\n",
            "Adding AutoShape... \n",
            "Inference Progress: 100%|██████████| 1320/1320 [10:37<00:00,  2.11it/s]"
          ]
        }
      ]
    }
  ]
}